## GFS论文阅读

原文地址：[GFS](https://pdos.csail.mit.edu/6.824/papers/gfs.pdf)

翻译版本地址：[黄规速大佬翻译版本](https://cloud.tencent.com/developer/beta/article/1981238)

> 提醒：读这些论文需要摈弃一些细枝末节的东西，更多地去体会其中奇妙构思和巧妙实现（讲师Robert Morris在Lecture1中提到）

小贴士：可以使用snipaste来截图贴图功能以达到同时阅读和记笔记的目的，效果如下图，个人觉得很方便。

![image-20230507170654406](https://gitee.com/moni_world/pic_bed/raw/master/img/image-20230507170654406.png)

### 摘要

GFS是谷歌研发的有别于传统分布式文件系统地文件系统，其中有很多相同之处，也有很多从思路上完全不一样的东西。

关键词：

**容错，可伸缩性，数据存储，集群存储**。



### 1. 简介 

#### 01 组件失效

被认为是常态事件，组件完全可能因为各种千奇百怪的bug而无法运行，且无法恢复，所以错误检测，状态监控，灾难冗余和自动恢复是GFS必须有的功能；

#### 02 文件大小

文件大小要大，因为我们通常会管理很多的大文件，如果文件大小为几十kb，就同时管理上亿个文件，显然不合理。

#### 03 对文件的写入

基本是在文件末尾追加，随机写几乎不存在。所以性能瓶颈主要在数据的追加操作。

#### 04 应用程序和文件系统API协同设计

降低了开发难度并提高了GFS的灵活性，同时引入了原子性的追加操作，保证多个客户端同时写，不需要额外的同步操作；



### 2. 设计概述

#### 2.1 设计预期

- 系统由许多廉价的组件构成，失效是常态，需要能够迅速的侦错并恢复（第一时间想到轮询，但是似乎效率太低，很期待后面如何解决）；

- 系统主要存储大文件，必须支持小文件，但并不需要为小文件做专门优化（我的猜测是一个适中大小的块，然后分散存，小文件存不满就不需要管？或者说多个小文件存在一个块里？）

- 系统的工作负载由两种读操作组成：

  1. 大规模的流式读取（1MB+）；
  2. 小规模的随机读取（几十kb）；

  如果系统对性能非常关注的，建议是将小规模随机读取排序然后按顺序读（防止文件指针来回移动）；

- 大规模的追加写操作，但也提供小规模随机写，可能效率不佳（这里能看出，根据实际系统的需要，我们可以在设计上对某些操作更偏袒一些，并不需要做到尽善尽美）。

- 系统必须高效且明确定义：以生产者-消费者进程为例，如果系统的生产者可能有成百上千个（同时对文件进行读写），所以原子操作必须要高效且精确，使用最小的开销来实现多路追加操作必不可少。

- 高性能的稳定网络比低延迟更重要，我们更需要大批量且高效地处理文件操作，而不是对单一文件地响应操作有需求。



#### 2.2 接口

GFS提供一套API接口函数，但并不完全遵循POSIX标准，以分层目录的方式组织，用路径名来标识，我们支持常用的文件操作（增删改查读写打开关闭）；

提供快照和记录追加操作，快照以一个很低的成本创建一个文件或目录树的拷贝（很好奇如何实现，应该是类似于指针或者引用的操作？）；记录追加操作：允许多个客户端同时对一个文件进行数据追加操作，同时保证操作都是原子性的，多个客户端在不需要额外锁的情况下可以同时进行写操作，这对于“生产者消费者”模型非常有用。



#### 2.3 架构

一个GFS集群包括一个单独的Master节点（前面的MapReduce也提到这样一个控制节点），多台Chunk服务器，同时被多客户访问，大致结构如下图：

![image-20230507171134939](https://gitee.com/moni_world/pic_bed/raw/master/img/image-20230507171134939.png)

GFS文件通常被分割成固定大小的chunk，创建之初就被分配一个全球唯一的64位标识；chunk服务器把Chunk以文件的方式保存，并根据标识和字节范围来读写块数据，处于可靠性，每个块被复制多次（但这样复制同步岂不是狠麻烦， 每次写要同步多次）。

**Master管理元数据，包括命名空间，访问权限，文件和Chunk的映射，以及Chunk的位置。**（数据结构已经出来了，放大一下方便后面做Lab参考），Master还管理系统范围内的活动，Chunk租用管理（？没懂），孤儿Chunk的回收，以及Chunk在Chunk服务器的转移。用心跳信息周期和Chunk服务器通讯，发送指令到Chunk接收Chunk服务器状态（果然是轮询！我的timerfd和epoll已经饥渴难耐了）

*GFS客户端代码以库的实行被连接到客户程序（麻烦的东西，不好调试），客户端代码实现了GFS文件系统的API接口函数、应用程序与Master节点和Chunk服务器通讯、以及对数据进行读写操作。客户端和Master节点的通信只获取元数据，所有的数据操作都是由客户端直接和Chunk服务器进行交互的。*==（数据结构的描述~，标记一下）==

客户端和Chunk都不需要缓存文件数据，客户端只会缓存元数据，Chunk以本地文件的方式保存，Linux系统会处理这些文件。



#### 2.4 单一Master节点

通过全局信息精确定位Chunk的位置及进行复制决策；必须减少对Master的读写，避免Master成为系统瓶颈（大并发？感觉一个Master很容易就成为瓶颈了啊），所以客户端并不通过Master节点读写文件数据，而是通过询问Master去联系Chunk服务器（Lab1的Master 分配给Worker），客户端仅需要缓存原信息一段时间，后续直接和Chunk服务器进行数据的读写；

一个小细节：Master返回的Chunk信息可能还有该Chunk紧跟的后面的Chunk信息，减少了客户端与Master的交互。

具体通信过程可以参见前面的图1。



#### 2.5 Chunk尺寸

**64MB**，是一个非常大的Block size，问题在于内存碎片会很大（例如63MB），它们以Linux文件的形式保存在了Chunk服务器上，只有需要的时候才会扩大。

优点如下：

- 因为Chunk很大，所以减少了Master需要交互的次数，试想如果是按照kb来算的话，一个原本64M的Chunk，本来只需要一次交互，但换成kb级别的Chunk大小可能要进行成千上万次与Master的交互，让Master压力山大。
- 减小了客户端的通信次数，让客户端维持长连接的状态（不然每次都要建立和断开TCP连接，三握四挥），减少了网络负载。
- 减少了Master节点需要保存的元数据数量，允许我们把元数据存在内存中（访问速度大大提升）。

但即使配合惰性空间分配，也有其缺点：

小文件只有很少的Chunk，多次访问可能让某个文件成为热点文件（性能瓶颈出现的地方）。虽然有其缺点，但在实际应用中，该缺点非常少出现，甚至可以忽略不计，不由感叹前人智慧，实际程序设计中需要合理规避缺点，并最大化利用优点。

解决该缺点两个小想法：

1. 对热点文件设计更大的复制参数，多保存几份拷贝；
2. 在该情况下，允许客户端从别的客户端读数据（负载均衡的思想，妙啊）。



#### 2.6 元数据

Master服务器（与节点不同）存储三种类型的元数据，**文件和Chunk的命名空间**，**文件和Chunk的关系**，**以及每个Chunk的副本存储地点** 。所有元数据保存在Master服务器的内存，前两种数据也会记录变更日志的方式记录在操作系统的系统日志文件，日志存储在本地磁盘上，同时日志会被复制到其他远程的Master服务器上，采用保存变更日志的方式，能够实现简单可靠的更新服务器的状态（学到了同步的方法，感觉有点像数据库的同步，先写日志，再写文件），也能预防Master崩溃导致数据不一致（更像数据库的设计了）。

Master不会持久保存Chunk的位置信息，在启动时，或有新的Chunk服务器加入时，向各个Chunk服务器轮询它们的Chunk信息。



##### 2.6.1 内存中的数据结构

元数据保存在内存中，所以服务器的速度非常之快；并且可以让Master服务器在后台简单而高效的周期性扫描自己的全部状态信息，这种周期性的状态扫描也用于实现Chunk垃圾回收，Chunk失效重新复制，通过Chunk的迁移实现跨Chunk的负载均衡和磁盘使用状况统计等功能。

将元数据全部保存在内存中也有其风险存在：

Chunk的数量以及系统的承受额能力都受限于Master服务器的内存大小，还是那句话，这些问题在实际应用中，并不是很严重，大多数文件包含多个Chunk，因此绝大多数Chunk都是满的，除了文件的最后一个Chunk是部分填充的，文件名也会根据前缀压缩；

即便需要支持更大的文件系统，我们也仅仅需要增加很少的Master服务器内存，增强了系统的简洁，可靠，高性能，灵活性。



##### 2.6.2 Chunk的位置信息

Master服务器并不持久化保存这Chunk的信息，而是轮询，让自己保存的Chunk信息始终是最新的。

简化了Master和Chunk服务器数据同步的问题，只有Chunk服务器才能确定Chunk所在的位置，而Master仅仅是作为一个反馈机制。



##### 2.6.3 操作日志

包含了关键的元数据变更的历史记录，它也是元数据唯一的持久化存储记录，也能通过它来判断同步操作的顺序（方便进行同步）。文件和Chunk以及它们的版本，都有他们创建的逻辑时间唯一的，永久的标识。

操作日志必须确保完整性，当元数据的操作被持久化后，日志对客户端菜是可见的，否则即使Chunk本神没问题，我们任然可能丢失文件系统或客户端，所以需要复制日志，只有日志写好以后，才会更新相应的日志记录，也才会响应客户端的操作请求。Master服务器会收集多个日志记录后批量处理，以减少写操作和复制对系统造成性能影响。

Master灾难恢复就是通过重演操作日志把文件恢复到最近的状态，为了缩短Master的启动时间，必须让日志尽可能地小，在日志增长到一定大小后做一次chenkpoint（类似于数据库快照，保存当前的状态，存个档），把当前状态保存到checkpoint文件中，同时删除之前的日志，如果崩溃，直接重演checkpoint就行了。

创建Checkpoint需要一定时间，所以Master服务器的内部状态被组织为一种格式，Master服务器使用独立线程切换到新的日志文件并创建Checkpoint文件，完成后写入到本地以及远程磁盘。

旧的日志和Checkpoint可以被删除，但为了防止灾难性故障（超出预期的灾难性故障，*catastrophes*）通常会多保存一点，Checkpoint是可以出错的，在容错机制内。



#### 2.7 一致性模型

命名空间的修改是原子性的，命名空间锁保证了原子性和正确性，Master节点的操作日志定义了这些操作在全局的顺序，节点的操作日志定义了这些操作的全局性；

##### 2.7.1 文件一致性的保障机制

数据修改后文件region的状态取决于该操作，可查看表1，来总结各种操作的结果如果所有客户端，无论从任何地方读取都是一致的，那么region就是“serial success”的，如果客户端能看到写的全部内容，那么就是“defined”，那么如果一个数据修改及操作成功执行，并且没有受到同时执行的其他写入操作的干扰，那么影响的region就是“defined（包含一致性）”；其他情况可以看下面的表格；



![image-20230507214300457](https://gitee.com/moni_world/pic_bed/raw/master/img/image-20230507214300457.png)

经过一系列成功的修改，GFS确保被修改的文件region是已定义的，并且包含最后一次修改写操作写入的数据，GFS通过以下措施确保上述行为：（a）对Chunk的所有副本修改操作顺序一致， （b）使用Chunk版本号，检测出是否因为服务器宕机而错过了修改操作。失效的副本不会再进行任何修改，Master服务器也不再返回这个Chunk副本的位置信息给客户端，他们会被回收。

Chunk的位置信息会被客户端缓存，客户端可能读取失效副本；文件再次被打开后会清楚缓存中与该文件相关的所有Chunk信息，由于文件大多数只进行追加操作，所以一个失效的副本通常返回一个提前结束的Chunk，当一个Reader重新尝试并联络master，立刻得到最新的Chunk位置信息。

即使修改操作成功后，依然可能损坏或删除数据，所以GFS通过Master和所有Chunk定期握手来找到失效的CHunk，并使用Chunksum校验数据是否损坏。Chunk在检测到错误并采取措施之前完全丢失，才会真正丢失，一般情况下（Master检错并修复）需要几分钟，这几分钟Chunk只是不可用，而不是损坏了；应用会收到明确错误而非损坏的数据。

##### 2.7.2 程序的实现

实际应用中，我们对文件更多是数据追加而非覆盖。



### 3. 系统交互

设计原则：最小化所有操作和Master节点的交互；



#### 3.1 租约（lease）和变更顺序

变更：写入和记录追加操作，变更再Chunk所有副本上执行，使用lease机制来保持多个副本间变更顺序的一致性。

Master节点会为其中一个Chunk副本创建一个租约，该副本就是主Chunk，主Chunk对所有Chunk的变更操作进行序列化，所有副本遵循这个序列进行修改操作。修改操作的全局顺序因此确定：

1. 优先Master选择的lease的顺序；
2. 然后lease中主Chunk分配的序列号来最终确定顺序；

租约机制就是为了最小化Master节点的管理负担设定的，只需要与主Chunk来确定其他副本的执行顺序。如下图展示了一个写入操作的执行顺序：

![image-20230508221845200](https://gitee.com/moni_world/pic_bed/raw/master/img/image-20230508221845200.png)

1. 客户机向Master节点询问lease被哪个Chunk持有以及该Chunk的位置。如果没有Chunk持有租约，Master节点就选其中一个副本建立租约（图上并未展示）；

2. Master节点返回主Chunk的标识符以及其他副本（第二副本）的位置返回给Client，客户机缓存，并断开Master的连接，只有主Chunk不可用或表明不再持有lease时，客户机才需要再次建立连接；

3. 客户机把数据推送到所有副本上（这个顺序并不固定），Chunk服务器接收数据并保存到内部LRU中，直到数据被使用或过期。由于数据流的网络传输负载很高，我们可以基于网络情况对数据流进行操作，而不用在意哪个Chunk保存了主Chunk（因为论文时间有点太早了，所以很容易看出该论文依然在规避网络传输，但实际现在的网络已经非常快，不再需要处处避免网络带宽了）。

4. 当所有确认收到数据，客户端给主Chunk发写请求，主Chunk分配序列号，然后在本地执行；

5. 再把所有写请求传递到所有二级副本，每个二级副本按照主Chunk分配的序列号以相同的顺序执行这些操作；

6. 所有二级副本告诉主Chunk，它们已完成操作；

7. 主Chunk服务器回复客户机（任何错误都会返回给客户），如果主Chunk出错，序列号不会生成，也不会被传递。

   客户端的请求被确认为失败，客户机代码通过重复执行是白的操作来处理这样的错误。从头开始重复执行之前，客户机麾下从3到7做几次尝试

如果写入数据很大，GFS会把他们分多次写，每次写都遵循这个写的过程。region的尾部可能包含来自不同客户机的数据片段，但至少他们都是以相同的顺序执行完成，所有副本都是一致的。（这里没太懂，有时间重新回来看看，但是先往下读着）



#### 3.2 数据流

为了提高网络效率，分开数据流和控制流。控制流从客户机到主Chunk，再到所有二级副本，此时数据以管道的方式顺序地沿着一个精心选择地Chunk服务器链推送。

基于TCP连接，管道式地推送数据（全双工可以同时收发）。



#### 3.3 原子的记录追加

类似于O_APPEND模式打开的文件，多个并发写操作再没有竞态条件的行为。

是一种修改操作，遵循3.1中描述的控制流程，除了在主Chunk有些额外的控制逻辑。客户机把数据推送给文件的最后一个Chunk，然后发消息给主Chunk，主C来检查是否超过Chunk的最大尺寸（64MB），如果超过，填充剩下，通知所有二级副本进行相同操作，然后再通知客户机对下一个Chunk记录追加（记录追加数据严格控制在Chunk的1/4）。

如果在任何i一个副本失效，都需要客户端重新操作。因此可能Chunk不同副本包含不同数据，GFS并不保证所有文件在字节级别完全一致，只保证数据作为一个整体原子至少被写入一次（本身副本只是为了同步而存在，有一个正确的副本就可以了，多的只是容错）。

有点理解了，就是说，不管写没写，偏移量已经记住了，继续往偏移量后写就完事了？



#### 3.4 快照

瞬间完成对一个文件或目录树的拷贝，并不会对其他操作造成干扰（到底是如何实现的呢？）

用标准的copy-on-write来实现快照，Master收到一个快照请求：

1. 取消所有的Chunk租约。保证了后续对Chunk的写操作必须与Master交互以找到lease（真没懂），让Master节点率先创建Chunk的新拷贝的机会。

2. 租约取消或国企，Master把操作写日志，通过复制元数据或源文件来把记录反映到保存在内存的状态中，新创建的快照和源文件指向完全相同的Chunk地址。

3. 快照操作后，客户机第一次想写到Chunk C，首先发送一个请求到Master节点查询当前租约持有者。 Master节点注意到Chunk的引用计数超过了1。

   额，真没看懂，等着看视频吧。。。



### 4. Master节点的操作

执行所有的命名空间操作，管理所有的Chunk副本；决定了Chunk的存储位置，创建新的Chunk和他的副本，保证Chunk被完全复制，在服务器之间进行负载均衡，回收不再使用的存储空间。

##### 4.1 名称空间管理和锁

Master节点操作很花时间，为了高效，我们允许多个操作同时进行，使用名称空间的region上的锁来保证执行的正确顺序。

不同于许多传统的文件系统，GFS没有阵地每个目录实现能够列出目录下所有文件的数据结构。也不支持文件和目录的链接。逻辑上，GFS的名称空间就是一个查找表，利用前缀压缩，表可以高效的存储在内存。

每个Master节点开始前需要获得一系列的锁。通过文件名锁的冲突来巧妙顺序执行。

同时为了避免死锁，按顺序获取锁，非常重要（这又如何保证？锁的目的是为了按顺序，但获取锁又要按顺序？）

获取锁的顺序：首先按名称空间的层次排序，在同一个层次内按字典顺序排序。



##### 4.2 副本的位置

多层布局结构，而非平面结构。

副本位置选择策略：最大化数据可靠性和可用性，最大化网络带宽利用率。为了实现两个目的，多个机架间分布存储Chunk的副本。保证一些副本在整个机架被破坏或掉线，依然可用。写操作必须和多个机架的设备进行网络通信。



##### 4.3 创建，重新复制，重新负载均衡

Chunk副本的作用：Chunk创建，重新复制，重新负载均衡。



##### 4.4 垃圾回收

惰性垃圾回收原则：只回收文件和Chunk级别。

###### 4.4.1 机制

删除也是操作日志，修改文件名隐藏文件，然后删除三天前的隐藏文件，常规扫描也会删除孤儿Chunk；

也就是延迟回收机制。

##### 4.5 过期失效的文件机制

保存版本号来区分当前副本和过期副本。

Master签订租约，就会通知最新的副本，并保存最高的版本号，例行的扫描会删除所有过期副本。



### 5.容错和诊断

最大的挑战之一，机器和硬盘可能会失效。



#### 5.1 高可用性

##### 5.1.1 快速恢复

无论机器是如何关闭，都会被设置为快速重启，可能客户机仅仅是感觉有一些颠簸（卡顿了一下），请求会超时然后重传请求就可以了。

##### 5.1.2 Chunk复制

Chunk是会自动复制的，缺省是3，当有Chunk服务器离线或者检测出了错误数据，就会立刻从已有副本中复制（意思是怎么都会存在n个副本）。

##### 5.1.3 Master服务器复制

为保证Master的准确性，Master服务器的状态和操作日志也需要复制。对Master操作成功的前提是：操作日志成功写到Master服务器的备节点和本机磁盘。

一个Master服务进程负责所有修改操作。失效后可以立刻重新启动，因为仅仅需要开一个线程来运行已有的备份。客户端使用规范的明字访问Master，这个名字类似DNS别名，因此也就可以在Master进程转到别的机器上执行，通过更改别名的实际访问新的master。（这里感觉非常巧妙，并不需要动客户端源代码，就只需要改一个别名的指向就能完成所有访问全部更新！）

影子Master，在主Master失效时提供只读访问，但该影子可能会慢1s左右（我感觉这让同步的实现变得复杂了），但应用程序是从Chunk服务器来获取数据，所以并不会感受到这种延迟（原来如此，那同步就并不是问题，反而加快了一些文件的访问）。

为了保证影子Master最新，它会读取正在进行的操作日志副本，平时表现得和Master一样，影子Master服务器也会定期和Chunk服务器握手来确定状态，只有在主Master因为创建和删除副本位置信息更新的时候才会和Master交流。

#### 5.2 数据完整性

Chunk服务器使用CheckSum来检查保存的数据是否损坏。我们跨越Chunk服务器比较副本来检验数据是否损坏并不实际。而且GFS并不保证副本们是完全一致的（仅仅是每个数据至少写了一遍）。所以每个Chunk必须独立维护CheckSum来校验自己的副本。

校验和会在数据返回之前校验，Chunk不会把错误信息传递给其他机器，如果某个块的Checksum不正确，就会返回错误信息。作为回应，Master会指定一个完整的副本去读数据，当新副本就绪后再删除旧副本（就副本反正也不用，为什么不直接删除再进行新副本的创建）。

校验和对读操作的性能影响很小（无论如何都要保证正确性，多线程也会付出代价来保证race condition不对结果出现影响）。Chunk服务器做校验和并不需要IO操作，可以同时进行IO和校验和。

校验和对追加写操作做了高度优化，只增量更新最后一个不完整的块的校验和，用追加的校验和来计算新的校验和，即使现在没检验出来，下次对块的读取也会检验出来，然后对这个块进行读取操作就会检查出数据已经损坏了。

Chunk服务器空闲就会扫描不活动的Chunk，能够发现很少被读取的Chunk是否完整，然后创建新的，正确的副本，再把损坏的副本删掉。避免了非活动的，已损坏的Chunk欺骗Master。

#### 5.3 诊断工具

通过日志我们能理解分布式系统的信息交互，日志记录关键信息：Chunk的启动关闭，RPC请求和回复。诊断日志可以随意删除，但在空间足够的情况下一般会保存。

RPC日志包括了网络上所有的请求和响应的详细记录，但是不包括读写的文件数据。通过匹配请求与回应，以及收集不同机器上的RPC日志记录，我们可以重演所有的消息交互来诊断问题。日志还用以跟踪负载测试和性能分析。



### 6. 度量

测试，本节应该不做笔记，看论文就是要抓住重点，细节交给项目来慢慢打磨。

![image-20230511211404450](https://gitee.com/moni_world/pic_bed/raw/master/img/image-20230511211404450.png)
